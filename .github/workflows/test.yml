---
name: Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  AMQP_CONNECTION_STRING: amqp://test:test@localhost:5672/
  NEO4J_URI: bolt://localhost:7687
  NEO4J_USER: neo4j
  NEO4J_PASSWORD: testpassword

jobs:
  # Determine what changed to optimize test runs
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      docker: ${{ steps.filter.outputs.docker }}
      workflows: ${{ steps.filter.outputs.workflows }}
    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ” Detect changed files
        uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            backend:
              - 'apollonia/**'
              - 'ingestor/**'
              - 'populator/**'
              - 'analyzer/**'
              - 'api/**'
              - 'tests/**'
              - 'pyproject.toml'
              - 'uv.lock'
              - '*.py'
            frontend:
              - 'frontend/**'
            docker:
              - '**/Dockerfile'
              - 'docker-compose*.yml'
            workflows:
              - '.github/workflows/**'
              - '.github/actions/**'

  # Python unit tests - parallelized by test type
  python-tests:
    name: Python Tests (${{ matrix.test-group }})
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.workflows == 'true'

    permissions:
      contents: read
      checks: write
      pull-requests: write

    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.12']
        test-group: ['unit', 'integration-unit']
        include:
          - test-group: 'unit'
            test-path: 'tests/unit'
            marks: 'not integration and not e2e'
          - test-group: 'integration-unit'
            test-path: 'tests/integration'
            marks: 'not e2e and not docker'

    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ”§ Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ matrix.python-version }}
          install-just: true
          cache-key-prefix: python-test-${{ matrix.test-group }}

      - name: ðŸ’¾ Cache test results
        uses: actions/cache@v4
        with:
          path: |
            .pytest_cache
            .coverage
            coverage.xml
            htmlcov
          key: test-results-${{ runner.os }}-py${{ matrix.python-version }}-${{ matrix.test-group }}-${{ github.sha }}
          restore-keys: |
            test-results-${{ runner.os }}-py${{ matrix.python-version }}-${{ matrix.test-group }}-

      - name: ðŸ§ª Run ${{ matrix.test-group }} tests with coverage
        run: |
          uv run pytest ${{ matrix.test-path }} \
            -m "${{ matrix.marks }}" \
            --cov \
            --cov-report=xml \
            --cov-report=html \
            --junit-xml=pytest-results-${{ matrix.test-group }}.xml \
            --maxfail=5 \
            --tb=short \
            -n auto

      - name: ðŸ“¤ Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: matrix.python-version == '3.12'
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml
          flags: ${{ matrix.test-group }}
          name: apollonia-${{ matrix.test-group }}-coverage
          fail_ci_if_error: false

      - name: ðŸ“¤ Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: pytest-results-${{ matrix.test-group }}-py${{ matrix.python-version }}
          path: |
            pytest-results-${{ matrix.test-group }}.xml
            htmlcov/
            .coverage

      - name: ðŸ“Š Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always() && matrix.python-version == '3.12'
        with:
          files: pytest-results-${{ matrix.test-group }}.xml
          comment_mode: failures
          check_name: Python ${{ matrix.test-group }} Test Results

  # Frontend tests - with better caching
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    needs: changes
    if: needs.changes.outputs.frontend == 'true' || needs.changes.outputs.workflows == 'true'

    permissions:
      contents: read
      checks: write

    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ”§ Setup frontend environment
        uses: ./.github/actions/setup-frontend-env
        with:
          node-version: '22'
          install-just: true

      - name: ðŸ’¾ Cache test results
        uses: actions/cache@v4
        with:
          path: |
            frontend/coverage
            frontend/.next/cache
          key: frontend-test-results-${{ runner.os }}-${{ github.sha }}
          restore-keys: |
            frontend-test-results-${{ runner.os }}-

      - name: ðŸ§ª Run tests with coverage
        working-directory: frontend
        run: |
          npm run lint
          npm run type-check
          npm run test:ci -- --coverage --maxWorkers=50%

      - name: ðŸ“¤ Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./frontend/coverage/lcov.info
          flags: frontend
          name: apollonia-frontend-coverage
          fail_ci_if_error: false

      - name: ðŸ“¤ Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-test-results
          path: frontend/coverage/

  # Integration tests with services - improved startup time
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [changes, python-tests]
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.docker == 'true' || needs.changes.outputs.workflows == 'true'

    permissions:
      contents: read
      checks: write

    services:
      rabbitmq:
        image: rabbitmq:3-management-alpine
        ports:
          - 5672:5672
          - 15672:15672
        env:
          RABBITMQ_DEFAULT_USER: test
          RABBITMQ_DEFAULT_PASS: test
        options: >-
          --health-cmd "rabbitmq-diagnostics ping"
          --health-interval 5s
          --health-timeout 3s
          --health-retries 10
          --health-start-period 10s

      postgres:
        image: postgres:15-alpine
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: apollonia_test
          POSTGRES_HOST_AUTH_METHOD: trust
        options: >-
          --health-cmd "pg_isready -U test"
          --health-interval 3s
          --health-timeout 2s
          --health-retries 10
          --health-start-period 5s

      neo4j:
        image: neo4j:5-community
        ports:
          - 7474:7474
          - 7687:7687
        env:
          NEO4J_AUTH: neo4j/testpassword
          NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
          NEO4J_dbms_memory_heap_max__size: '512M'
          NEO4J_dbms_memory_heap_initial__size: '512M'
        options: >-
          --health-cmd "neo4j status || exit 1"
          --health-interval 5s
          --health-timeout 3s
          --health-retries 15
          --health-start-period 30s

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 3s
          --health-timeout 2s
          --health-retries 10
          --health-start-period 5s

    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ”§ Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.12'
          install-just: true
          cache-key-prefix: python-integration

      - name: ðŸ’¾ Cache integration test data
        uses: actions/cache@v4
        with:
          path: |
            tests/fixtures/data
            .pytest_cache
          key: integration-test-data-${{ runner.os }}-${{ hashFiles('tests/fixtures/**') }}

      - name: â³ Wait for services (optimized)
        run: |
          echo "â³ Waiting for services to be ready..."

          # Parallel health checks
          check_service() {
            local service=$1
            local check_cmd=$2
            local max_attempts=30
            local attempt=0

            while [ $attempt -lt $max_attempts ]; do
              if eval "$check_cmd" > /dev/null 2>&1; then
                echo "âœ… $service is ready"
                return 0
              fi
              attempt=$((attempt + 1))
              sleep 1
            done

            echo "âŒ $service failed to start"
            return 1
          }

          # Run health checks in parallel
          check_service "RabbitMQ" "curl -f http://localhost:15672" &
          check_service "PostgreSQL" "pg_isready -h localhost -p 5432 -U test" &
          check_service "Neo4j" "curl -f http://localhost:7474" &
          check_service "Redis" "redis-cli -h localhost ping" &

          # Wait for all checks to complete
          wait

          echo "âœ… All services are ready"

      - name: ðŸ”— Run integration tests
        env:
          POSTGRES_URL: postgresql://test:test@localhost:5432/apollonia_test
          REDIS_URL: redis://localhost:6379/0
        run: |
          uv run pytest tests/integration \
            -m "integration and not e2e" \
            --cov \
            --cov-report=xml \
            --junit-xml=integration-results.xml \
            --maxfail=5 \
            --tb=short \
            -n auto

      - name: ðŸ“¤ Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            integration-results.xml
            coverage.xml

  # E2E tests - with Docker layer caching
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [changes, integration-tests, frontend-tests]
    if: |
      (needs.changes.outputs.backend == 'true' ||
       needs.changes.outputs.frontend == 'true' ||
       needs.changes.outputs.docker == 'true' ||
       needs.changes.outputs.workflows == 'true') &&
      needs.integration-tests.result == 'success' &&
      needs.frontend-tests.result == 'success'

    permissions:
      contents: read
      checks: write
      packages: read

    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4

      - name: ðŸ—ï¸ Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: ðŸ”§ Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.12'
          install-just: true
          cache-key-prefix: python-e2e

      - name: ðŸ”§ Setup frontend environment
        uses: ./.github/actions/setup-frontend-env
        with:
          node-version: '22'

      - name: ðŸ’¾ Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: buildx-${{ runner.os }}-${{ github.sha }}
          restore-keys: |
            buildx-${{ runner.os }}-

      - name: ðŸ‹ Build services with cache
        run: |
          docker buildx bake \
            --set "*.cache-from=type=local,src=/tmp/.buildx-cache" \
            --set "*.cache-to=type=local,dest=/tmp/.buildx-cache-new,mode=max" \
            --load

      - name: ðŸŒ Run E2E tests
        run: |
          # Start services
          docker-compose up -d

          # Wait for services to be healthy
          timeout 60s bash -c 'until docker-compose ps | grep -E "(healthy|running)" | wc -l | grep -q "$(docker-compose ps -q | wc -l)"; do sleep 2; done'

          # Run E2E tests
          just test-e2e

      - name: ðŸ“‹ Show service logs on failure
        if: failure()
        run: |
          echo "ðŸ” Service logs:"
          docker-compose logs --tail=100

      - name: ðŸ›‘ Stop services
        if: always()
        run: docker-compose down -v

      - name: ðŸ“¤ Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            e2e-results.xml
            cypress/screenshots/
            cypress/videos/

      - name: ðŸ—‘ï¸ Move cache
        if: always()
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

  # Test summary - runs in parallel with other final jobs
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [python-tests, frontend-tests, integration-tests, e2e-tests]
    if: always()

    steps:
      - name: ðŸ“Š Test Results Summary
        run: |
          echo "## ðŸ§ª Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status | Duration |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|----------|" >> $GITHUB_STEP_SUMMARY
          echo "| Python Unit Tests | ${{ needs.python-tests.result == 'success' && 'âœ… Passed' || needs.python-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Frontend Tests | ${{ needs.frontend-tests.result == 'success' && 'âœ… Passed' || needs.frontend-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && 'âœ… Passed' || needs.integration-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| E2E Tests | ${{ needs.e2e-tests.result == 'success' && 'âœ… Passed' || needs.e2e-tests.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Performance tips based on results
          if [[ "${{ needs.python-tests.result }}" == "skipped" ]]; then
            echo "ðŸ’¡ **Performance Tip**: Python tests were skipped because no backend changes were detected." >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "${{ needs.frontend-tests.result }}" == "skipped" ]]; then
            echo "ðŸ’¡ **Performance Tip**: Frontend tests were skipped because no frontend changes were detected." >> $GITHUB_STEP_SUMMARY
          fi

  # Coverage report aggregation - parallel with summary
  coverage-report:
    name: Coverage Report
    runs-on: ubuntu-latest
    needs: [python-tests, frontend-tests, integration-tests]
    if: always() && github.event_name == 'pull_request'

    permissions:
      pull-requests: write

    steps:
      - name: ðŸ“¥ Download all coverage reports
        uses: actions/download-artifact@v4
        with:
          pattern: '*-results*'
          merge-multiple: true

      - name: ðŸ“Š Generate coverage summary
        run: |
          echo "## ðŸ“Š Coverage Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Coverage reports have been uploaded to Codecov for detailed analysis." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "View detailed coverage: [Codecov Dashboard](https://codecov.io/gh/${{ github.repository }})" >> $GITHUB_STEP_SUMMARY

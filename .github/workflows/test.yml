---
name: Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  AMQP_CONNECTION_STRING: amqp://test:test@localhost:5672/
  NEO4J_URI: bolt://localhost:7687
  NEO4J_USER: neo4j
  NEO4J_PASSWORD: apollonia

jobs:
  # Determine what changed to optimize test runs
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      docker: ${{ steps.filter.outputs.docker }}
      workflows: ${{ steps.filter.outputs.workflows }}
    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üîç Detect changed files
        uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            backend:
              - 'apollonia/**'
              - 'ingestor/**'
              - 'populator/**'
              - 'analyzer/**'
              - 'api/**'
              - 'tests/**'
              - 'pyproject.toml'
              - 'uv.lock'
              - '*.py'
            frontend:
              - 'frontend/**'
            docker:
              - '**/Dockerfile'
              - 'docker-compose*.yml'
            workflows:
              - '.github/workflows/**'
              - '.github/actions/**'

  # Python unit tests - parallelized by test type
  python-tests:
    name: Python Tests (${{ matrix.test-group }})
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: changes
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.workflows == 'true'

    permissions:
      contents: read
      checks: write
      pull-requests: write

    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.12']
        test-group: ['unit', 'integration-unit']
        include:
          - test-group: 'unit'
            test-path: 'tests/unit'
            marks: 'not integration and not e2e'
          - test-group: 'integration-unit'
            test-path: 'tests/integration'
            marks: 'not e2e and not docker'

    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üîß Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: ${{ matrix.python-version }}
          install-just: true
          cache-key-prefix: python-test-${{ matrix.test-group }}

      - name: üíæ Cache test results
        uses: actions/cache@v4
        with:
          path: |
            .pytest_cache
            .coverage
            coverage.xml
            htmlcov
          key: test-results-${{ runner.os }}-py${{ matrix.python-version }}-${{ matrix.test-group }}-${{ github.sha }}
          restore-keys: |
            test-results-${{ runner.os }}-py${{ matrix.python-version }}-${{ matrix.test-group }}-

      - name: üß™ Run ${{ matrix.test-group }} tests with coverage
        timeout-minutes: 10
        run: |
          just test-python-unit-ci "${{ matrix.test-path }}" "${{ matrix.marks }}"
          # Rename the output file to match the matrix test group
          mv junit.xml pytest-results-${{ matrix.test-group }}.xml

      - name: üì§ Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        if: matrix.python-version == '3.12'
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./coverage.xml
          flags: ${{ matrix.test-group }}
          name: apollonia-${{ matrix.test-group }}-coverage
          fail_ci_if_error: false

      - name: üì§ Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: pytest-results-${{ matrix.test-group }}-py${{ matrix.python-version }}
          path: |
            pytest-results-${{ matrix.test-group }}.xml
            htmlcov/
            .coverage

      - name: üìä Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always() && matrix.python-version == '3.12'
        with:
          files: pytest-results-${{ matrix.test-group }}.xml
          comment_mode: failures
          check_name: Python ${{ matrix.test-group }} Test Results

  # Frontend tests - with better caching
  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: changes
    if: needs.changes.outputs.frontend == 'true' || needs.changes.outputs.workflows == 'true'

    permissions:
      contents: read
      checks: write

    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üîß Setup frontend environment
        uses: ./.github/actions/setup-frontend-env
        with:
          node-version: '22'
          install-just: true

      - name: üíæ Cache test results
        uses: actions/cache@v4
        with:
          path: |
            frontend/coverage
            frontend/.next/cache
          key: frontend-test-results-${{ runner.os }}-${{ github.sha }}
          restore-keys: |
            frontend-test-results-${{ runner.os }}-

      - name: üß™ Run tests with coverage
        timeout-minutes: 8
        run: just test-frontend-ci

      - name: üì§ Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          file: ./frontend/coverage/lcov.info
          flags: frontend
          name: apollonia-frontend-coverage
          fail_ci_if_error: false

      - name: üì§ Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: frontend-test-results
          path: |
            frontend/coverage/
            frontend/test-report.junit.xml

  # Integration tests with services - improved startup time
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [changes, python-tests]
    if: needs.changes.outputs.backend == 'true' || needs.changes.outputs.docker == 'true' || needs.changes.outputs.workflows == 'true'

    permissions:
      contents: read
      checks: write

    services:
      rabbitmq:
        image: rabbitmq:3-management-alpine
        ports:
          - 5672:5672
          - 15672:15672
        env:
          RABBITMQ_DEFAULT_USER: test
          RABBITMQ_DEFAULT_PASS: test
        options: >-
          --health-cmd "rabbitmq-diagnostics ping"
          --health-interval 5s
          --health-timeout 3s
          --health-retries 10
          --health-start-period 10s

      postgres:
        image: postgres:15-alpine
        ports:
          - 5432:5432
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: apollonia_test
          POSTGRES_HOST_AUTH_METHOD: trust
        options: >-
          --health-cmd "pg_isready -U test"
          --health-interval 3s
          --health-timeout 2s
          --health-retries 10
          --health-start-period 5s

      neo4j:
        image: neo4j:5-community
        ports:
          - 7474:7474
          - 7687:7687
        env:
          NEO4J_AUTH: neo4j/apollonia
          NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
          NEO4J_dbms_memory_heap_max__size: '512M'
          NEO4J_dbms_memory_heap_initial__size: '512M'
        options: >-
          --health-cmd "neo4j status || exit 1"
          --health-interval 5s
          --health-timeout 3s
          --health-retries 15
          --health-start-period 30s

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping || exit 1"
          --health-interval 5s
          --health-timeout 5s
          --health-retries 20
          --health-start-period 10s

    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üîß Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.12'
          install-just: true
          cache-key-prefix: python-integration

      - name: üíæ Cache integration test data
        uses: actions/cache@v4
        with:
          path: |
            tests/fixtures/data
            .pytest_cache
          key: integration-test-data-${{ runner.os }}-${{ hashFiles('tests/fixtures/**') }}

      - name: ‚è≥ Wait for services (optimized)
        run: |
          echo "‚è≥ Waiting for services to be ready..."

          # Parallel health checks
          check_service() {
            local service=$1
            local check_cmd=$2
            local max_attempts=30
            local attempt=0

            while [ $attempt -lt $max_attempts ]; do
              if eval "$check_cmd" > /dev/null 2>&1; then
                echo "‚úÖ $service is ready"
                return 0
              fi
              attempt=$((attempt + 1))
              sleep 1
            done

            echo "‚ùå $service failed to start"
            return 1
          }

          # Run health checks in parallel
          check_service "RabbitMQ" "curl -f http://localhost:15672" &
          check_service "PostgreSQL" "pg_isready -h localhost -p 5432 -U test" &
          check_service "Neo4j" "curl -f http://localhost:7474" &
          check_service "Redis" "redis-cli -h localhost ping" &

          # Wait for all checks to complete
          wait

          echo "‚úÖ All services are ready"

      - name: üîó Run integration tests
        timeout-minutes: 15
        env:
          POSTGRES_URL: postgresql://test:test@localhost:5432/apollonia_test
          REDIS_URL: redis://localhost:6379/0
        run: just test-python-integration-ci

      - name: üì§ Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: |
            junit.xml
            coverage.xml

  # E2E tests - with Docker layer caching
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [changes, integration-tests, frontend-tests]
    if: |
      (needs.changes.outputs.backend == 'true' ||
       needs.changes.outputs.frontend == 'true' ||
       needs.changes.outputs.docker == 'true' ||
       needs.changes.outputs.workflows == 'true') &&
      needs.integration-tests.result == 'success' &&
      needs.frontend-tests.result == 'success'

    permissions:
      contents: read
      checks: write
      packages: read

    steps:
      - name: üì• Checkout repository
        uses: actions/checkout@v4

      - name: üèóÔ∏è Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: üîß Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.12'
          install-just: true
          cache-key-prefix: python-e2e

      - name: üîß Setup frontend environment
        uses: ./.github/actions/setup-frontend-env
        with:
          node-version: '22'

      - name: üêã Install docker-compose
        uses: alexellis/arkade-get@1eef818e467c387d3f50cfe0d2c565d1cbe82b03 # master
        with:
          docker-compose: latest

      - name: üíæ Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: buildx-${{ runner.os }}-${{ github.sha }}
          restore-keys: |
            buildx-${{ runner.os }}-

      - name: üêã Build services with cache
        timeout-minutes: 20
        run: |
          # Use docker-compose build instead of buildx bake to avoid privilege issues
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          docker-compose build \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --parallel

      - name: üåê Run E2E tests
        timeout-minutes: 15
        run: |
          # Start services
          docker-compose up -d

          # Wait for services to be healthy
          timeout 60s bash -c 'until docker-compose ps | grep -E "(healthy|running)" | wc -l | grep -q "$(docker-compose ps -q | wc -l)"; do sleep 2; done'

          # Run E2E tests
          just test-e2e

      - name: üìã Show service logs on failure
        if: failure()
        run: |
          echo "üîç Service logs:"
          docker-compose logs --tail=100

      - name: üõë Stop services
        if: always()
        run: docker-compose down -v

      - name: üì§ Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: |
            e2e-results.xml
            cypress/screenshots/
            cypress/videos/

      - name: üóëÔ∏è Move cache
        if: always()
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

  # Test summary - runs in parallel with other final jobs
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [python-tests, frontend-tests, integration-tests, e2e-tests]
    if: always()

    steps:
      - name: üìä Test Results Summary
        run: |
          echo "## üß™ Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status | Duration |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|----------|" >> $GITHUB_STEP_SUMMARY
          echo "| Python Unit Tests | ${{ needs.python-tests.result == 'success' && '‚úÖ Passed' || needs.python-tests.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Frontend Tests | ${{ needs.frontend-tests.result == 'success' && '‚úÖ Passed' || needs.frontend-tests.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && '‚úÖ Passed' || needs.integration-tests.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| E2E Tests | ${{ needs.e2e-tests.result == 'success' && '‚úÖ Passed' || needs.e2e-tests.result == 'skipped' && '‚è≠Ô∏è Skipped' || '‚ùå Failed' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Performance tips based on results
          if [[ "${{ needs.python-tests.result }}" == "skipped" ]]; then
            echo "üí° **Performance Tip**: Python tests were skipped because no backend changes were detected." >> $GITHUB_STEP_SUMMARY
          fi
          if [[ "${{ needs.frontend-tests.result }}" == "skipped" ]]; then
            echo "üí° **Performance Tip**: Frontend tests were skipped because no frontend changes were detected." >> $GITHUB_STEP_SUMMARY
          fi

  # Coverage report aggregation - parallel with summary
  coverage-report:
    name: Coverage Report
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [python-tests, frontend-tests, integration-tests]
    if: always() && github.event_name == 'pull_request'

    permissions:
      pull-requests: write

    steps:
      - name: üì• Download all coverage reports
        uses: actions/download-artifact@v4
        with:
          pattern: '*-results*'
          merge-multiple: true

      - name: üìä Generate coverage summary
        run: |
          echo "## üìä Coverage Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Coverage reports have been uploaded to Codecov for detailed analysis." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "View detailed coverage: [Codecov Dashboard](https://codecov.io/gh/${{ github.repository }})" >> $GITHUB_STEP_SUMMARY
